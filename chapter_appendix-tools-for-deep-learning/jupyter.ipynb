{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdeshYadav8918/Smart-Sleep-Analytics-Automated-Apnea-Detection-using-AI/blob/master/chapter_appendix-tools-for-deep-learning/jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smart Sleep Analytics: Automated Apnea Detection using AI\n",
        "**Comprehensive Google Colab notebook** (Academic style)\n",
        "**Includes:** Synthetic data generation, optional PhysioNet data instructions, preprocessing, feature extraction, classical ML (Random Forest, SVM, Logistic Regression), deep models (CNN, LSTM), evaluation, visualizations, and discussion.\n",
        "\n",
        "**Author:** Adesh Yadav\n",
        "**Date:** October 2025\n",
        "\n",
        "---\n",
        "**How to use:**\n",
        "\n",
        "- Run the cells sequentially in Google Colab.\n",
        "- Use the Synthetic Data path to run immediately.\n",
        " - Or follow the PhysioNet section to download real data (requires internet and larger runtime).\n"
      ],
      "metadata": {
        "id": "-3o2vVzEFJRp"
      },
      "id": "-3o2vVzEFJRp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Install required packages\n",
        "The following cell installs necessary packages. In Colab some are preinstalled, but we include them for completeness."
      ],
      "metadata": {
        "id": "mcXV7r0dFWJ8"
      },
      "id": "mcXV7r0dFWJ8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages (uncomment if needed in Colab)\n",
        "# Note: TensorFlow is usually preinstalled in Colab; adjust version if required.\n",
        "!pip install -q wfdb==4.1.1  # for PhysioNet (optional)\n",
        "!pip install -q tensorflow==2.19.0 scikit-learn matplotlib pandas seaborn joblib"
      ],
      "metadata": {
        "id": "WLjcc9BeDck4"
      },
      "id": "WLjcc9BeDck4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Imports and helper functions\n"
      ],
      "metadata": {
        "id": "N40Ucm0AGPxj"
      },
      "id": "N40Ucm0AGPxj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard imports\n",
        "import os, random, math, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import signal, stats\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# ensure reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "print('numpy', np.__version__, 'pandas', pd.__version__, 'tf', tf.__version__)\n"
      ],
      "metadata": {
        "id": "DVHqPAPZDc06"
      },
      "id": "DVHqPAPZDc06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Synthetic data generation (recommended to run immediately)\n",
        "This generates respiratory and SpO2-like signals and injects apnea events. Use this to run the whole pipeline without external data."
      ],
      "metadata": {
        "id": "JGkECnBHGYD0"
      },
      "id": "JGkECnBHGYD0"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic(duration_sec=900, fs=100, apnea_rate_per_min=0.8):\n",
        "    \"\"\"Generate synthetic respiratory and SpO2 signals with labeled apnea events.\n",
        "    Returns a pandas DataFrame with columns ['timestamp','resp','spo2','label']\"\"\"\n",
        "    t = np.arange(0, duration_sec, 1/fs)\n",
        "    # respiratory signal: base sine (breathing ~0.25 Hz) + small noise\n",
        "    resp = 0.6 * np.sin(2*np.pi*0.25*t) + 0.05*np.random.randn(len(t))\n",
        "    # SpO2 baseline with slow fluctuation and measurement noise\n",
        "    spo2 = 98 + 0.8 * np.sin(2*np.pi*0.05*t) + 0.2*np.random.randn(len(t))\n",
        "    labels = np.zeros_like(t, dtype=int)\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    # Insert apnea events (randomly)\n",
        "    num_events = int(duration_sec/60 * apnea_rate_per_min)\n",
        "    for _ in range(num_events):\n",
        "        start = rng.randint(0, len(t)-fs*20)\n",
        "        length = rng.randint(int(fs*5), int(fs*15))  # 5-15 seconds\n",
        "        resp[start:start+length] *= rng.uniform(0.05,0.2)  # suppressed breathing amplitude\n",
        "        spo2[start:start+length] -= rng.uniform(2,8)        # SpO2 dip\n",
        "        labels[start:start+length] = 1\n",
        "    df = pd.DataFrame({'timestamp': t, 'resp': resp, 'spo2': spo2, 'label': labels})\n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "df = generate_synthetic(duration_sec=900, fs=100)\n",
        "df.head(), df.shape\n"
      ],
      "metadata": {
        "id": "gUooPNuwDdA4"
      },
      "id": "gUooPNuwDdA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quick plot of signals (first 60 seconds)"
      ],
      "metadata": {
        "id": "CmrUjrhsGivT"
      },
      "id": "CmrUjrhsGivT"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(df['timestamp'][:6000], df['resp'][:6000])\n",
        "plt.title('Respiratory signal (first 60s)')\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(df['timestamp'][:6000], df['spo2'][:6000])\n",
        "plt.title('SpO2 signal (first 60s)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jkW1vgtFDdNM"
      },
      "id": "jkW1vgtFDdNM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Segment the signals into windows for ML\n",
        "We segment into overlapping windows and label a window as positive if a fraction of samples inside have apnea label >= threshold."
      ],
      "metadata": {
        "id": "GdvBiJvlGnxq"
      },
      "id": "GdvBiJvlGnxq"
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_signal(df, window_size=300, step_size=150, cols=['resp','spo2'], label_col='label', threshold=0.5):\n",
        "    X = []\n",
        "    y = []\n",
        "    n = len(df)\n",
        "    for start in range(0, n - window_size + 1, step_size):\n",
        "        window = df.iloc[start:start+window_size]\n",
        "        X.append(window[cols].values)\n",
        "        lbl = (window[label_col].mean() >= threshold).astype(int)\n",
        "        y.append(lbl)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Parameters (3s windows at 100Hz -> 300 samples -> change if needed)\n",
        "WINDOW_SIZE = 300\n",
        "STEP_SIZE = 150\n",
        "CHANNELS = ['resp','spo2']\n",
        "\n",
        "X_segments, y = segment_signal(df, window_size=WINDOW_SIZE, step_size=STEP_SIZE, cols=CHANNELS, label_col='label', threshold=0.1)\n",
        "print('Segments shape:', X_segments.shape, 'Labels distribution:', np.bincount(y))\n"
      ],
      "metadata": {
        "id": "kA-JygPcDdao"
      },
      "id": "kA-JygPcDdao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) Feature extraction (statistical + spectral)\n",
        "We extract stats (mean,std,skew,kurtosis) and spectral band powers via Welch."
      ],
      "metadata": {
        "id": "L3huocuoGygG"
      },
      "id": "L3huocuoGygG"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats, signal\n",
        "\n",
        "def stat_features(window):\n",
        "    feats = []\n",
        "    for c in range(window.shape[1]):\n",
        "        col = window[:, c]\n",
        "        feats.extend([np.mean(col), np.std(col), np.min(col), np.max(col),\n",
        "                      np.median(col), np.percentile(col,25), np.percentile(col,75),\n",
        "                      stats.skew(col), stats.kurtosis(col)])\n",
        "    return np.array(feats)\n",
        "\n",
        "def bandpower(x, fs, fmin, fmax):\n",
        "    f, Pxx = signal.welch(x, fs=fs, nperseg=min(256, len(x)))\n",
        "    mask = (f >= fmin) & (f <= fmax)\n",
        "    return np.trapz(Pxx[mask], f[mask]) if np.any(mask) else 0.0\n",
        "\n",
        "def spectral_features(window, fs=100.0):\n",
        "    feats = []\n",
        "    for c in range(window.shape[1]):\n",
        "        col = window[:, c]\n",
        "        feats.append(bandpower(col, fs, 0.1, 0.5))  # very low freq\n",
        "        feats.append(bandpower(col, fs, 0.5, 2))    # low freq\n",
        "        feats.append(bandpower(col, fs, 2, 5))      # respiratory band\n",
        "    return np.array(feats)\n",
        "\n",
        "def extract_features_from_segments(X_segments, fs=100.0):\n",
        "    rows = []\n",
        "    for w in X_segments:\n",
        "        s = stat_features(w)\n",
        "        p = spectral_features(w, fs=fs)\n",
        "        rows.append(np.concatenate([s,p]))\n",
        "    cols = [f'f{i}' for i in range(len(rows[0]))]\n",
        "    return pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "# Extract features\n",
        "X_feat_df = extract_features_from_segments(X_segments, fs=100.0)\n",
        "X_feat_df.shape, X_feat_df.head()\n"
      ],
      "metadata": {
        "id": "UlZBdXStEAA9"
      },
      "id": "UlZBdXStEAA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6) Train classical ML models (feature-based)\n",
        "We train RandomForest, SVM, and Logistic Regression and evaluate using stratified split."
      ],
      "metadata": {
        "id": "sBTKB8bOGzeY"
      },
      "id": "sBTKB8bOGzeY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X = X_feat_df.values\n",
        "y_lbl = y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_lbl, test_size=0.2, random_state=SEED, stratify=y_lbl)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_s = scaler.transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=150, random_state=SEED),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=SEED),\n",
        "    'LogisticRegression': LogisticRegression(max_iter=200, random_state=SEED)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, m in models.items():\n",
        "    print('\\nTraining', name)\n",
        "    m.fit(X_train_s, y_train)\n",
        "    preds = m.predict(X_test_s)\n",
        "    probs = m.predict_proba(X_test_s)[:,1] if hasattr(m, 'predict_proba') else None\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    prec = precision_score(y_test, preds, zero_division=0)\n",
        "    rec = recall_score(y_test, preds, zero_division=0)\n",
        "    f1 = f1_score(y_test, preds, zero_division=0)\n",
        "    auc = roc_auc_score(y_test, probs) if probs is not None else None\n",
        "    results[name] = {'model': m, 'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'auc': auc}\n",
        "    print(f'ACC={acc:.3f}, PREC={prec:.3f}, REC={rec:.3f}, F1={f1:.3f}, AUC={auc if auc is not None else \"N/A\"}')\n",
        "\n",
        "# Save scaler and best model (by F1)\n",
        "best_name = max(results.keys(), key=lambda k: results[k]['f1'])\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "joblib.dump(results[best_name]['model'], f'{best_name}_model.joblib')\n",
        "print('\\nSaved best classical model:', best_name)\n"
      ],
      "metadata": {
        "id": "1xIyLpEpEASV"
      },
      "id": "1xIyLpEpEASV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Deep learning models (end-to-end on windows)\n",
        "Train a small LSTM and 1D-CNN on the raw windows (shape: N, T, C)."
      ],
      "metadata": {
        "id": "eNNPoj9SG9S3"
      },
      "id": "eNNPoj9SG9S3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for deep models\n",
        "X_deep = X_segments.astype('float32')\n",
        "y_deep = y.astype('float32')\n",
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_deep, y_deep, test_size=0.2, random_state=SEED, stratify=y_deep)\n",
        "\n",
        "timesteps = X_train_d.shape[1]; n_features = X_train_d.shape[2]\n",
        "print('Deep input shape', X_train_d.shape)\n",
        "\n",
        "# Correctly import layers and models from tensorflow.keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "def build_lstm(timesteps, n_features):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(timesteps, n_features)),\n",
        "        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(32)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_cnn(timesteps, n_features):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(timesteps, n_features)),\n",
        "        layers.Conv1D(64, kernel_size=5, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Training LSTM (light training for demo)\n",
        "lstm = build_lstm(timesteps, n_features)\n",
        "es = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "history_lstm = lstm.fit(X_train_d, y_train_d, epochs=20, batch_size=32, validation_split=0.1, callbacks=[es], verbose=2)\n",
        "\n",
        "# Evaluate LSTM\n",
        "probs_lstm = lstm.predict(X_test_d).ravel()\n",
        "preds_lstm = (probs_lstm > 0.5).astype(int)\n",
        "print('LSTM - acc:', accuracy_score(y_test_d, preds_lstm), 'f1:', f1_score(y_test_d, preds_lstm, zero_division=0))"
      ],
      "metadata": {
        "id": "qwIx_YPFEAkK"
      },
      "id": "qwIx_YPFEAkK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training CNN (light training for demo)\n",
        "cnn = build_cnn(timesteps, n_features)\n",
        "history_cnn = cnn.fit(X_train_d, y_train_d, epochs=20, batch_size=32, validation_split=0.1, callbacks=[es], verbose=2)\n",
        "\n",
        "# Evaluate CNN\n",
        "probs_cnn = cnn.predict(X_test_d).ravel()\n",
        "preds_cnn = (probs_cnn > 0.5).astype(int)\n",
        "print('CNN - acc:', accuracy_score(y_test_d, preds_cnn), 'f1:', f1_score(y_test_d, preds_cnn, zero_division=0))\n",
        "\n",
        "# Save models\n",
        "lstm.save('lstm_model.h5')\n",
        "cnn.save('cnn_model.h5')\n",
        "print('Saved LSTM and CNN models.')\n"
      ],
      "metadata": {
        "id": "xqN4BrhGEAtp"
      },
      "id": "xqN4BrhGEAtp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#8) Evaluation & Visualizations\n",
        "Confusion matrices, ROC curves, and short discussion."
      ],
      "metadata": {
        "id": "-dRAnmaZHEwf"
      },
      "id": "-dRAnmaZHEwf"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title='Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Classical best model evaluation\n",
        "best_clf = joblib.load(f'{best_name}_model.joblib')\n",
        "X_test_s = scaler.transform(X_test)\n",
        "probs_clf = best_clf.predict_proba(X_test_s)[:,1] if hasattr(best_clf, 'predict_proba') else None\n",
        "preds_clf = best_clf.predict(X_test_s)\n",
        "print('Best classical model:', best_name)\n",
        "print('Metrics:', 'acc', accuracy_score(y_test, preds_clf), 'f1', f1_score(y_test, preds_clf, zero_division=0))\n",
        "plot_confusion(y_test, preds_clf, title=f'{best_name} Confusion Matrix')\n",
        "\n",
        "# LSTM/CNN confusion matrices\n",
        "plot_confusion(y_test_d, preds_lstm, title='LSTM Confusion Matrix')\n",
        "plot_confusion(y_test_d, preds_cnn, title='CNN Confusion Matrix')\n",
        "\n",
        "# ROC curves\n",
        "plt.figure(figsize=(8,6))\n",
        "if probs_clf is not None:\n",
        "    fpr, tpr, _ = roc_curve(y_test, probs_clf)\n",
        "    plt.plot(fpr, tpr, label=f'{best_name} (AUC={auc(fpr,tpr):.3f})')\n",
        "fpr, tpr, _ = roc_curve(y_test_d, probs_lstm)\n",
        "plt.plot(fpr, tpr, label=f'LSTM (AUC={auc(fpr,tpr):.3f})')\n",
        "fpr, tpr, _ = roc_curve(y_test_d, probs_cnn)\n",
        "plt.plot(fpr, tpr, label=f'CNN (AUC={auc(fpr,tpr):.3f})')\n",
        "plt.plot([0,1],[0,1],'--', color='gray')\n",
        "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.title('ROC Curves'); plt.show()\n"
      ],
      "metadata": {
        "id": "Pr6P_CNcEBAR"
      },
      "id": "Pr6P_CNcEBAR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9) (Optional) Using real datasets from PhysioNet\n",
        "This section explains how to download and use PhysioNet Sleep-EDF or Apnea-ECG datasets. Running this in Colab requires internet access and may take time.\n",
        "\n",
        "Note: Data must be preprocessed into aligned CSV of channels used (resp, spo2, etc.) with a label column (0/1)."
      ],
      "metadata": {
        "id": "s1gTuWC-HO5W"
      },
      "id": "s1gTuWC-HO5W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: using wfdb to download a record (user must adapt for dataset and labeling)\n",
        "# Uncomment and modify record names as required.\n",
        "# import wfdb\n",
        "# record = wfdb.rdrecord('apnea-database-1.0.0/records/xxxx')  # adjust path\n",
        "# print(record.sig_name, record.fs, record.p_signal.shape)\n",
        "# Convert signals to pandas and then follow segmentation/feature extraction steps\n"
      ],
      "metadata": {
        "id": "wqdU2bm7HaOM"
      },
      "id": "wqdU2bm7HaOM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10) Save artifacts & Next steps\n",
        "Models (scaler.joblib, RandomForest_model.joblib, lstm_model.h5, cnn_model.h5) are saved in the working directory.\n",
        "Next: try with real PhysioNet data, add explainability (SHAP), or deploy as a Flask app for inference."
      ],
      "metadata": {
        "id": "gwVnoXdZHfl4"
      },
      "id": "gwVnoXdZHfl4"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "# List saved files\n",
        "import os\n",
        "print('Files in workspace:')\n",
        "for f in os.listdir('.')[:50]:\n",
        "    if any(ext in f for ext in ['.h5','.joblib','.png']):\n",
        "        print('-', f)\n",
        "print('\\nYou can download models from the Colab Files sidebar.')\n"
      ],
      "metadata": {
        "id": "omANQuLhEBp-"
      },
      "id": "omANQuLhEBp-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Files in workspace:\n",
        "- SVM_model.joblib\n",
        "- lstm_model.h5\n",
        "- scaler.joblib\n",
        "- cnn_model.h5\n",
        "\n",
        "You can download models from the Colab Files sidebar.\n"
      ],
      "metadata": {
        "id": "OgKd-1rYH4Hl"
      },
      "id": "OgKd-1rYH4Hl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}